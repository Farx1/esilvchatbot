import { NextRequest, NextResponse } from 'next/server'
import { db } from '@/lib/db'

interface ScraperResult {
  title: string
  content: string
  url: string
  confidence: number
  date?: string  // Date de publication de l'actualit√©
  fullContent?: string  // Contenu complet de la page
  category?: string  // Cat√©gorie de l'actualit√©
  tags?: string[]  // Tags/√©tiquettes
}

class ESILVWebScraper {
  private readonly baseUrl = 'https://www.esilv.fr'
  
  async scrapeESILVInfo(query: string, currentDate?: Date, deepScrape: boolean = true): Promise<ScraperResult[]> {
    const results: ScraperResult[] = []
    
    // D√©tecter si c'est une question sur l'actualit√©
    const isNewsQuery = /\b(actualit√©|actualit√©s|news|dernier|derni√®re|r√©cent|nouveau)\b/i.test(query)
    
    try {
      if (isNewsQuery) {
        // Pour les actualit√©s, scraper la page actualit√©s
        console.log('üì∞ Scraping page actualit√©s ESILV...')
        const newsResults = await this.scrapeNewsPage(currentDate, deepScrape)
        if (newsResults.length > 0) {
          results.push(...newsResults)
        } else {
          // Fallback to mock news data
          const mockResults = this.generateMockNewsData(currentDate)
          results.push(...mockResults)
        }
      } else {
        // Pour les autres questions, recherche g√©n√©rale
        const realResults = await this.realWebScrape(query)
        if (realResults.length > 0) {
          results.push(...realResults)
        } else {
          const mockResults = this.generateMockScrapedData(query)
          results.push(...mockResults)
        }
      }
      
    } catch (error) {
      console.error('Error scraping ESILV website:', error)
      // Fallback to mock data
      if (isNewsQuery) {
        const mockResults = this.generateMockNewsData(currentDate)
        results.push(...mockResults)
      } else {
        const mockResults = this.generateMockScrapedData(query)
        results.push(...mockResults)
      }
    }
    
    return results
  }

  private async scrapeNewsPage(currentDate?: Date, deepScrape: boolean = true): Promise<ScraperResult[]> {
    const results: ScraperResult[] = []
    
    try {
      // URL correcte de la page actualit√©s ESILV
      const newsUrl = `${this.baseUrl}/actus/`
      
      console.log(`üì∞ √âtape 1: Scraping liste actualit√©s: ${newsUrl}`)
      
      const response = await fetch(newsUrl, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        },
      })
      
      if (!response.ok) {
        console.log(`‚ö†Ô∏è HTTP ${response.status}, passage au mock data`)
        throw new Error(`HTTP error! status: ${response.status}`)
      }
      
      const html = await response.text()
      console.log(`‚úÖ Page liste charg√©e (${html.length} caract√®res)`)
      
      // Extraire les actualit√©s (titres, dates, URLs)
      const newsItems = this.extractNewsFromHTML(html, currentDate)
      
      if (newsItems.length === 0) {
        console.log('‚ö†Ô∏è Aucune actualit√© extraite')
        throw new Error('No news extracted')
      }
      
      console.log(`üì∞ ${newsItems.length} actualit√©s extraites`)
      
      // √âtape 2 : Deep scraping - Visiter chaque page d'actualit√© pour le contenu complet
      if (deepScrape) {
        console.log(`\nüî¨ √âtape 2: Deep scraping de ${newsItems.length} articles...`)
        
        for (let i = 0; i < newsItems.length; i++) {
          const item = newsItems[i]
          
          try {
            console.log(`  üìÑ Article ${i+1}/${newsItems.length}: ${item.title.substring(0, 50)}...`)
            
            // Extraire l'URL de l'article depuis le lien
            const fullContent = await this.scrapeArticlePage(item.url)
            
            if (fullContent) {
              item.fullContent = fullContent
              item.content = fullContent.substring(0, 500) + '...' // R√©sum√©
              item.confidence = 0.95 // Confiance plus √©lev√©e car contenu complet
              console.log(`    ‚úÖ Contenu r√©cup√©r√© (${fullContent.length} caract√®res)`)
            } else {
              console.log(`    ‚ö†Ô∏è Contenu non r√©cup√©r√©, utilisation de l'extrait`)
            }
            
            // D√©lai entre chaque requ√™te pour √©viter de surcharger le serveur
            await new Promise(resolve => setTimeout(resolve, 500))
            
          } catch (error) {
            console.log(`    ‚ùå Erreur: ${error}`)
            // Garder l'extrait initial si le deep scraping √©choue
          }
        }
        
        console.log(`‚úÖ Deep scraping termin√©\n`)
      }
      
      results.push(...newsItems)
      
    } catch (error) {
      console.error('Real news scraping failed:', error)
    }
    
    return results
  }

  private async scrapeArticlePage(articleUrl: string): Promise<string> {
    try {
      const response = await fetch(articleUrl, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
          'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        },
      })
      
      if (!response.ok) {
        return ''
      }
      
      const html = await response.text()
      
      // Extraire le contenu principal de l'article
      // Sur ESILV, le contenu est dans des balises <p> dans le corps de l'article
      
      // M√©thode 1 : Chercher le contenu entre les balises sp√©cifiques
      let content = ''
      
      // Extraire tous les paragraphes
      const paragraphRegex = /<p[^>]*>([\s\S]*?)<\/p>/gi
      const paragraphs: string[] = []
      let match
      
      while ((match = paragraphRegex.exec(html)) !== null) {
        const cleanParagraph = match[1]
          .replace(/<[^>]+>/g, '') // Enlever les tags HTML
          .replace(/&nbsp;/g, ' ')
          .replace(/&amp;/g, '&')
          .replace(/&quot;/g, '"')
          .replace(/&#039;/g, "'")
          .replace(/&lt;/g, '<')
          .replace(/&gt;/g, '>')
          .replace(/\s+/g, ' ')
          .trim()
        
        // Garder les paragraphes de plus de 50 caract√®res (filtre le bruit)
        if (cleanParagraph.length > 50) {
          paragraphs.push(cleanParagraph)
        }
      }
      
      // Prendre les 5 premiers paragraphes significatifs
      content = paragraphs.slice(0, 5).join(' ')
      
      // Limiter √† 1500 caract√®res pour le prompt
      if (content.length > 1500) {
        content = content.substring(0, 1500) + '...'
      }
      
      return content
      
    } catch (error) {
      console.error(`Error scraping article page: ${error}`)
      return ''
    }
  }

  private extractNewsFromHTML(html: string, currentDate?: Date): ScraperResult[] {
    const results: ScraperResult[] = []
    
    try {
      console.log('üîç Extraction des actualit√©s depuis la structure ESILV...')
      
      // Structure ESILV sp√©cifique : <div class="post_wrapper one_third">
      // Avec <div class="post_date"> pour la date et <h5><a> pour le titre
      
      // Pattern pour extraire les blocs d'actualit√©s complets
      const postBlockPattern = /<div class="post_wrapper[^"]*">([\s\S]*?)<\/div>\s*<\/div>\s*(?:<br class="clear">)?/gi
      let blockMatch
      let newsExtracted = 0
      
      while ((blockMatch = postBlockPattern.exec(html)) !== null && newsExtracted < 6) { // Augment√© √† 6 articles
        const block = blockMatch[1]
        
        // Extraire la date (jour, mois, ann√©e)
        const dateMatch = /<div class="date">(\d+)<\/div>[\s\S]*?<div class="month">(\w+)<\/div>[\s\S]*?<div class="year">(\d{4})<\/div>/i.exec(block)
        let newsDate = currentDate?.toLocaleDateString('fr-FR') || ''
        
        if (dateMatch) {
          const day = dateMatch[1]
          const month = dateMatch[2]
          const year = dateMatch[3]
          newsDate = `${day} ${month} ${year}`
        }
        
        // Extraire le titre depuis <h5><a>
        const titleMatch = /<h5[^>]*><a[^>]*title="([^"]*)"[^>]*>([^<]+)/i.exec(block)
        let title = ''
        
        if (titleMatch) {
          title = (titleMatch[1] || titleMatch[2]).trim()
          // Nettoyer le titre
          title = title.replace(/&quot;/g, '"')
                       .replace(/&#039;/g, "'")
                       .replace(/&amp;/g, '&')
                       .replace(/\s+/g, ' ')
                       .trim()
        }
        
        // Extraire l'URL de l'article depuis le <h5><a href>
        const urlMatch = /<h5[^>]*><a href="([^"]+)"[^>]*title=/i.exec(block)
        let articleUrl = urlMatch ? urlMatch[1] : ''
        
        // Si pas d'URL trouv√©e, chercher dans tout le bloc
        if (!articleUrl) {
          const altUrlMatch = /<a href="([^"]+)"[^>]*title="[^"]*"[^>]*>/i.exec(block)
          articleUrl = altUrlMatch ? altUrlMatch[1] : `${this.baseUrl}/actus/`
        }
        
        // S'assurer que l'URL est compl√®te
        if (articleUrl && !articleUrl.startsWith('http')) {
          articleUrl = `${this.baseUrl}${articleUrl.startsWith('/') ? '' : '/'}${articleUrl}`
        }
        
        console.log(`  üîó URL extraite: ${articleUrl}`)
        
        // Extraire un extrait du contenu depuis <div class="post_excerpt">
        const excerptMatch = /<div class="post_excerpt[^"]*">[\s\S]*?<p>([^<]+)<\/p>/i.exec(block)
        let excerpt = ''
        
        if (excerptMatch) {
          excerpt = excerptMatch[1]
            .replace(/&nbsp;/g, ' ')
            .replace(/&amp;/g, '&')
            .replace(/&quot;/g, '"')
            .replace(/\s+/g, ' ')
            .trim()
            .substring(0, 200)
        }
        
        // Extraire les tags/√©tiquettes
        const tagsMatch = /√âtiquettes&nbsp;:([^<]*)/i.exec(block)
        const tags: string[] = []
        
        if (tagsMatch) {
          const tagLinks = tagsMatch[1].match(/rel="tag">([^<]+)<\/a>/gi) || []
          tagLinks.forEach(tagLink => {
            const tag = tagLink.match(/rel="tag">([^<]+)<\/a>/i)
            if (tag) {
              tags.push(tag[1].trim())
            }
          })
        }
        
        // Filtrer les titres g√©n√©riques ou trop courts
        const isGeneric = /^(en savoir plus|demandez|nos brochures|contactez|t√©l√©charger|√©v√©nement)/i.test(title)
        
        if (title && title.length > 20 && !isGeneric) {
          results.push({
            title: title,
            content: excerpt || `Actualit√© ESILV du ${newsDate}: ${title}. Pour plus de d√©tails, consultez le site officiel de l'ESILV.`,
            url: articleUrl,
            confidence: 0.80, // Sera augment√© √† 0.95 apr√®s deep scraping
            date: newsDate,
            tags: tags.length > 0 ? tags : undefined
          })
          newsExtracted++
          console.log(`‚úÖ Actualit√© ${newsExtracted}: "${title.substring(0, 50)}..." (${newsDate})`)
          if (tags.length > 0) {
            console.log(`   üè∑Ô∏è  Tags: ${tags.join(', ')}`)
          }
        }
      }
      
      if (results.length === 0) {
        console.log('‚ö†Ô∏è Aucune actualit√© extraite avec la m√©thode principale, tentative alternative...')
        
        // M√©thode alternative : rechercher tous les h5 avec dates
        const h5Pattern = /<h5[^>]*><a[^>]*>([^<]+)<\/a><\/h5>/gi
        const datePattern = /(\d{1,2})\s+(Jan|F√©v|Mar|Avr|Mai|Juin|Juil|Ao√ªt|Sep|Oct|Nov|D√©c)\s+(\d{4})/gi
        
        const titles: string[] = []
        const dates: string[] = []
        
        let h5Match
        while ((h5Match = h5Pattern.exec(html)) !== null) {
          const title = h5Match[1].trim()
          if (title.length > 20) {
            titles.push(title)
          }
        }
        
        let dateMatch
        while ((dateMatch = datePattern.exec(html)) !== null) {
          dates.push(`${dateMatch[1]} ${dateMatch[2]} ${dateMatch[3]}`)
        }
        
        console.log(`üì∞ M√©thode alternative: ${titles.length} titres, ${dates.length} dates`)
        
        const count = Math.min(titles.length, dates.length, 3)
        for (let i = 0; i < count; i++) {
          results.push({
            title: titles[i],
            content: `Actualit√© ESILV du ${dates[i]}: ${titles[i]}. Pour plus de d√©tails, consultez https://www.esilv.fr/actus/`,
            url: `${this.baseUrl}/actus/`,
            confidence: 0.75,
            date: dates[i]
          })
        }
      }
      
      console.log(`üìä Total: ${results.length} actualit√©s extraites avec succ√®s`)
      
    } catch (error) {
      console.error('Error extracting news:', error)
    }
    
    return results
  }

  private generateMockNewsData(currentDate?: Date): ScraperResult[] {
    const dateStr = currentDate?.toLocaleDateString('fr-FR') || new Date().toLocaleDateString('fr-FR')
    const month = currentDate?.toLocaleDateString('fr-FR', { month: 'long', year: 'numeric' }) || 'd√©cembre 2024'
    
    return [
      {
        title: 'ESILV : Nouveau partenariat avec des entreprises du secteur Tech',
        content: `L'ESILV annonce en ${month} de nouveaux partenariats avec des leaders du secteur technologique. Ces collaborations permettront aux √©tudiants de b√©n√©ficier de stages, d'alternances et de projets r√©els en entreprise, renfor√ßant ainsi leur employabilit√© d√®s la sortie de l'√©cole.`,
        url: `${this.baseUrl}/actualites/partenariats-tech-2024`,
        confidence: 0.85,
        date: dateStr
      },
      {
        title: 'Lancement de nouveaux projets de recherche appliqu√©e',
        content: `L'√©cole lance plusieurs projets de recherche appliqu√©e en ${month} dans les domaines de l'IA, de la cybers√©curit√© et du d√©veloppement durable. Ces projets, men√©s en collaboration avec des industriels, permettent aux √©tudiants de travailler sur des probl√©matiques concr√®tes.`,
        url: `${this.baseUrl}/actualites/recherche-appliquee`,
        confidence: 0.80,
        date: dateStr
      },
      {
        title: 'Succ√®s des √©tudiants ESILV aux comp√©titions nationales',
        content: `Les √©quipes d'√©tudiants ESILV se sont illustr√©es r√©cemment lors de plusieurs comp√©titions nationales en ing√©nierie et innovation. Ces succ√®s t√©moignent de l'excellence de la formation et de l'engagement des √©tudiants.`,
        url: `${this.baseUrl}/actualites/competitions-2024`,
        confidence: 0.75,
        date: dateStr
      }
    ]
  }

  private async realWebScrape(query: string): Promise<ScraperResult[]> {
    const results: ScraperResult[] = []
    
    try {
      // Use fetch to get the page content
      const searchUrl = `${this.baseUrl}/recherche?q=${encodeURIComponent(query)}`
      
      const response = await fetch(searchUrl, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (compatible; ESILVBot/1.0)',
        },
      })
      
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }
      
      const html = await response.text()
      
      // Basic text extraction without external libraries
      const textContent = this.extractTextFromHTML(html, query)
      
      if (textContent) {
        results.push({
          title: `Information ESILV sur "${query}"`,
          content: textContent,
          url: searchUrl,
          confidence: 0.75
        })
      }
      
    } catch (error) {
      console.error('Real scraping failed:', error)
    }
    
    return results
  }

  private extractTextFromHTML(html: string, query: string): string {
    // Remove scripts and styles
    let text = html.replace(/<script[^>]*>[\s\S]*?<\/script>/gi, '')
    text = text.replace(/<style[^>]*>[\s\S]*?<\/style>/gi, '')
    
    // Remove HTML tags
    text = text.replace(/<[^>]+>/g, ' ')
    
    // Decode HTML entities
    text = text.replace(/&nbsp;/g, ' ')
    text = text.replace(/&amp;/g, '&')
    text = text.replace(/&lt;/g, '<')
    text = text.replace(/&gt;/g, '>')
    text = text.replace(/&quot;/g, '"')
    
    // Clean up whitespace
    text = text.replace(/\s+/g, ' ').trim()
    
    // Find relevant sections (simple keyword matching)
    const queryWords = query.toLowerCase().split(' ')
    const sentences = text.split(/[.!?]+/)
    
    const relevantSentences = sentences.filter(sentence => {
      const lowerSentence = sentence.toLowerCase()
      return queryWords.some(word => lowerSentence.includes(word))
    }).slice(0, 3) // Take first 3 relevant sentences
    
    return relevantSentences.join('. ').substring(0, 500)
  }
  
  private generateMockScrapedData(query: string): ScraperResult[] {
    // This would be replaced with actual web scraping logic
    const lowerQuery = query.toLowerCase()
    
    if (lowerQuery.includes('admission') || lowerQuery.includes('postuler')) {
      return [{
        title: 'Proc√©dure d\'admission √† l\'ESILV',
        content: 'Les admissions √† l\'ESILV se font principalement par le concours Puissance Alpha pour les √©l√®ves de Terminale. Les √©tudiants peuvent √©galement int√©grer l\'√©cole en admission parall√®le apr√®s un Bac+2/3/4.',
        url: `${this.baseUrl}/admissions`,
        confidence: 0.85
      }]
    }
    
    if (lowerQuery.includes('frais') || lowerQuery.includes('co√ªt') || lowerQuery.includes('prix')) {
      return [{
        title: 'Frais de scolarit√© ESILV',
        content: 'Les frais de scolarit√© √† l\'ESILV varient selon le statut de l\'√©tudiant. Pour les √©tudiants en formation initiale, comptez environ 8500‚Ç¨ par an. Des bourses et aides financi√®res sont disponibles.',
        url: `${this.baseUrl}/vie-etudiante/financement`,
        confidence: 0.80
      }]
    }
    
    return [{
      title: 'Information ESILV',
      content: `Recherche sur "${query}" - Informations disponibles sur le site officiel de l'ESILV.`,
      url: `${this.baseUrl}/recherche?q=${encodeURIComponent(query)}`,
      confidence: 0.60
    }]
  }
  
  async saveToKnowledgeBase(result: ScraperResult, category: string = 'scraped'): Promise<void> {
    try {
      // Utiliser le contenu complet si disponible, sinon l'extrait
      const contentToSave = result.fullContent || result.content
      
      // Cr√©er une question format√©e
      const question = `${result.title} (${result.date || 'Date inconnue'})`
      
      // Cr√©er une r√©ponse format√©e avec toutes les infos
      let answer = contentToSave
      
      if (result.tags && result.tags.length > 0) {
        answer += `\n\nTags: ${result.tags.join(', ')}`
      }
      
      answer += `\n\nSource: ${result.url}`
      
      await db.knowledgeBase.create({
        data: {
          question: question,
          answer: answer,
          category: category,
          confidence: result.confidence,
          source: result.url
        }
      })
    } catch (error) {
      console.error('Error saving scraped data:', error)
    }
  }
}

export async function POST(request: NextRequest) {
  try {
    const { query, autoSave = false, currentDate, deepScrape = true } = await request.json()
    
    if (!query) {
      return NextResponse.json(
        { error: 'Query is required' },
        { status: 400 }
      )
    }
    
    const date = currentDate ? new Date(currentDate) : new Date()
    console.log(`\n${'='.repeat(60)}`)
    console.log(`üîç SCRAPER POST: "${query}"`)
    console.log(`üìÖ Date: ${date.toLocaleDateString('fr-FR')}`)
    console.log(`üî¨ Deep Scraping: ${deepScrape ? 'Activ√©' : 'D√©sactiv√©'}`)
    console.log('='.repeat(60) + '\n')
    
    const scraper = new ESILVWebScraper()
    const results = await scraper.scrapeESILVInfo(query, date, deepScrape)
    
    console.log(`\n${'='.repeat(60)}`)
    console.log(`‚úÖ R√âSULTATS: ${results.length} actualit√©s trouv√©es`)
    if (results.length > 0 && results[0].date) {
      console.log(`üìÖ Dates: ${results.map(r => r.date).filter(Boolean).join(', ')}`)
    }
    
    // V√©rifier si les actualit√©s existent d√©j√† dans le RAG
    let newArticles = 0
    let existingArticles = 0
    
    if (autoSave) {
      console.log('\nüíæ Sauvegarde dans le RAG...')
      
      for (const result of results) {
        // V√©rifier si l'article existe d√©j√†
        const existing = await db.knowledgeBase.findFirst({
          where: {
            OR: [
              { question: { contains: result.title } },
              { answer: { contains: result.title } }
            ]
          }
        })
        
        if (!existing) {
          await scraper.saveToKnowledgeBase(result, 'web_scraped')
          newArticles++
          console.log(`  ‚úÖ Nouveau: "${result.title.substring(0, 50)}..."`)
        } else {
          existingArticles++
          console.log(`  ‚è≠Ô∏è  Existe d√©j√†: "${result.title.substring(0, 50)}..."`)
        }
      }
      
      console.log(`\nüìä Sauvegarde: ${newArticles} nouveaux, ${existingArticles} existants`)
    }
    console.log('='.repeat(60) + '\n')
    
    return NextResponse.json({
      success: true,
      results,
      count: results.length,
      savedToKB: autoSave,
      newArticles: autoSave ? newArticles : undefined,
      existingArticles: autoSave ? existingArticles : undefined
    })
  } catch (error) {
    console.error('Scraper API error:', error)
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    )
  }
}

export async function GET() {
  return NextResponse.json({
    message: 'ESILV Web Scraper API',
    usage: 'POST /api/scraper with { query: string, autoSave: boolean }',
    status: 'active'
  })
}

