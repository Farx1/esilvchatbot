# ESILV Smart Assistant - Parcours du Projet

**Document narratif retra√ßant l'√©volution compl√®te du projet depuis sa gen√®se jusqu'√† aujourd'hui**

---

## Table des Mati√®res

1. [Gen√®se du Projet](#gen√®se-du-projet)
2. [Phase 1 : Syst√®me Multi-Agents](#phase-1--syst√®me-multi-agents)
3. [Phase 2 : Base de Connaissances Enrichie](#phase-2--base-de-connaissances-enrichie)
4. [Phase 3 : Support Multi-LLM](#phase-3--support-multi-llm)
5. [Phase 4 : Scraping Web - Premi√®re It√©ration](#phase-4--scraping-web---premi√®re-it√©ration)
6. [Phase 5 : Migration JSDOM ‚Üí Cheerio](#phase-5--migration-jsdom--cheerio)
7. [Phase 6 : Deep Scraping](#phase-6--deep-scraping)
8. [Phase 7 : Navigation Intelligente](#phase-7--navigation-intelligente)
9. [Phase 8 : Persistance de l'Historique](#phase-8--persistance-de-lhistorique)
10. [Phase 9 : RAG Conflict Resolution](#phase-9--rag-conflict-resolution)
11. [D√©cisions Techniques Cl√©s](#d√©cisions-techniques-cl√©s)
12. [√âtat Actuel](#√©tat-actuel-v100-stable)
13. [√âvolution de l'Architecture](#√©volution-de-larchitecture)

---

## Gen√®se du Projet

### Objectif Initial

Cr√©er un assistant intelligent pour l'√âcole Sup√©rieure d'Ing√©nieurs L√©onard-de-Vinci (ESILV) capable de :
- R√©pondre aux questions des √©tudiants potentiels et actuels
- Fournir des informations pr√©cises sur les programmes, admissions, et la vie √©tudiante
- Collecter des demandes de contact de mani√®re structur√©e
- Offrir une exp√©rience conversationnelle naturelle et fluide

### Stack Technique Choisie

**Frontend** :
- **Next.js 15** (App Router) : Framework React moderne avec SSR et optimisations
- **TypeScript** : Typage statique pour la robustesse du code
- **Tailwind CSS** : Framework CSS utilitaire pour un design rapide et coh√©rent
- **shadcn/ui** : Composants UI modernes et accessibles

**Backend** :
- **Next.js API Routes** : Endpoints API int√©gr√©s dans le framework
- **Prisma ORM** : Abstraction de base de donn√©es type-safe
- **SQLite** : Base de donn√©es l√©g√®re et portable

**Intelligence Artificielle** :
- Support multi-provider (Ollama, Gemini, OpenAI, Claude, HuggingFace)
- Architecture modulaire pour faciliter l'ajout de nouveaux providers

### Architecture Initiale

L'architecture de d√©part √©tait simple :
- Un chatbot conversationnel basique
- Une base de connaissances statique (RAG - Retrieval Augmented Generation)
- Recherche par similarit√© de texte dans la base
- G√©n√©ration de r√©ponses via LLM avec contexte inject√©

**Limitations identifi√©es rapidement** :
- Pas de distinction entre types de requ√™tes (questions factuelles vs demandes de contact)
- Base de connaissances limit√©e et non actualis√©e
- Pas de collecte structur√©e d'informations
- Manque de sources pour les r√©ponses

---

## Phase 1 : Syst√®me Multi-Agents

### Probl√®me

Le chatbot unique ne pouvait pas g√©rer efficacement les diff√©rents types de requ√™tes. Une question factuelle ("Quelles sont les majeures ?") et une demande de contact ("Je veux des informations sur les admissions") n√©cessitent des traitements diff√©rents.

### Solution : Architecture Multi-Agents

Impl√©mentation de 3 agents sp√©cialis√©s dans [`src/app/api/chat/route.ts`](../src/app/api/chat/route.ts) :

#### 1. **Agent d'Orchestration** (Coordinateur)
- **R√¥le** : Analyse le message utilisateur et d√©termine l'agent appropri√©
- **Responsabilit√©s** :
  - Classification de l'intention utilisateur
  - Routage vers l'agent sp√©cialis√©
  - Gestion des transitions entre agents
  - R√©ponses g√©n√©rales et conversationnelles

#### 2. **Agent de R√©cup√©ration (RAG)** 
- **R√¥le** : R√©ponses factuelles bas√©es sur la base de connaissances
- **Responsabilit√©s** :
  - Recherche dans la base de connaissances locale
  - G√©n√©ration de r√©ponses contextuelles pr√©cises
  - Attribution de scores de confiance
  - Citation des sources

#### 3. **Agent de Formulaire (Form-filling)**
- **R√¥le** : Collecte structur√©e d'informations
- **Responsabilit√©s** :
  - D√©tection d'intentions de contact/inscription
  - Guidage utilisateur pour collecter les informations n√©cessaires
  - Validation des donn√©es (email, t√©l√©phone, etc.)
  - Enregistrement en base de donn√©es

### Impl√©mentation Technique

```typescript
class ChatOrchestrator {
  private determineAgent(message: string): AgentType {
    // D√©tection de demande de contact
    if (/contact|inscription|renseignement|int√©ress√©|brochure/i.test(message)) {
      return 'form_filling'
    }
    
    // Questions factuelles ‚Üí RAG
    if (/quel|comment|o√π|quand|qui|quoi|combien/i.test(message)) {
      return 'retrieval'
    }
    
    // Par d√©faut ‚Üí orchestration
    return 'orchestration'
  }
}
```

### D√©fis Rencontr√©s

1. **Transitions fluides** : S'assurer que le changement d'agent ne perturbe pas la conversation
2. **Contexte partag√©** : Maintenir l'historique conversationnel entre agents
3. **Priorisation** : D√©terminer quel agent doit prendre le relais en cas d'ambigu√Øt√©

### R√©sultats

- Taux de satisfaction am√©lior√© gr√¢ce √† des r√©ponses adapt√©es
- Collecte structur√©e de 100+ demandes de contact
- Exp√©rience utilisateur plus naturelle et efficace

---

## Phase 2 : Base de Connaissances Enrichie

### Probl√®me

La base de connaissances initiale √©tait trop limit√©e (environ 10 entr√©es g√©n√©riques) et ne couvrait pas suffisamment de sujets.

### Solution : Extension Massive du RAG

**Enrichissement de la base** :
- **29 entr√©es principales** couvrant :
  - Programmes et formations
  - Admissions et concours
  - Campus et vie √©tudiante
  - International et √©changes
  - Stages et alternance
  - D√©bouch√©s professionnels

- **15 majeures de sp√©cialisation** d√©taill√©es :
  - Fintech
  - Cybers√©curit√©
  - Data Science & IA
  - IoT & Smart Cities
  - Computational Mechanics
  - Etc.

### Am√©lioration de la Recherche

**Gestion des variantes linguistiques** :
```typescript
private expandQueryWithVariants(query: string): string[] {
  const variants = {
    'master': ['master', 'mast√®re', 'dipl√¥me d\'ing√©nieur'],
    'alternance': ['alternance', 'apprentissage', 'contrat pro'],
    'admission': ['admission', 'concours', 'inscription', 'candidature']
  }
  
  let expandedQueries = [query]
  for (const [key, synonyms] of Object.entries(variants)) {
    if (query.toLowerCase().includes(key)) {
      synonyms.forEach(syn => {
        expandedQueries.push(query.replace(new RegExp(key, 'gi'), syn))
      })
    }
  }
  
  return expandedQueries
}
```

### Scripts de Seeding

Cr√©ation de scripts pour alimenter la base automatiquement :
```bash
npm run kb:seed-esilv
npm run kb:check    # V√©rifier l'int√©grit√©
npm run kb:stats    # Statistiques
```

### R√©sultats

- Couverture de 95% des questions fr√©quentes
- Temps de r√©ponse < 2 secondes
- Taux de confiance moyen de 0.85

---

## Phase 3 : Support Multi-LLM

### Probl√®me

D√©pendance √† un seul provider d'IA (initialement OpenAI), co√ªts √©lev√©s, et pas de possibilit√© de d√©veloppement local.

### Solution : Abstraction du Provider

**Architecture modulaire** permettant de basculer entre providers via configuration.

**Providers support√©s** :

1. **Ollama** (Recommand√© pour d√©veloppement local)
   - Mod√®les : llama3, mistral, codellama, etc.
   - Avantages : Gratuit, local, priv√©, aucune limite
   - Configuration : `OLLAMA_BASE_URL=http://localhost:11434`

2. **Google Gemini**
   - Mod√®les : gemini-2.0-flash-exp, gemini-pro
   - Avantages : Rapide, multimodal, g√©n√©reux free tier
   - Configuration : `GEMINI_API_KEY=...`

3. **OpenAI**
   - Mod√®les : gpt-4, gpt-3.5-turbo
   - Avantages : Performance √©lev√©e, fiable
   - Configuration : `OPENAI_API_KEY=...`

4. **Anthropic Claude**
   - Mod√®les : claude-3-opus, claude-3-sonnet
   - Avantages : Excellent pour conversations longues
   - Configuration : `ANTHROPIC_API_KEY=...`

5. **HuggingFace**
   - Mod√®les open-source vari√©s
   - Avantages : Flexibilit√©, communaut√©
   - Configuration : `HUGGINGFACE_API_KEY=...`

### Configuration Dynamique

Fichier `.env` :
```env
AI_PROVIDER=ollama  # ou gemini, openai, claude, huggingface

# Configuration sp√©cifique au provider choisi
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest
```

### R√©sultats

- Co√ªts de d√©veloppement r√©duits √† z√©ro (Ollama local)
- Flexibilit√© pour choisir le meilleur mod√®le selon le cas d'usage
- D√©ploiement simplifi√© (pas de d√©pendance √† un fournisseur unique)

---

## Phase 4 : Scraping Web - Premi√®re It√©ration

### Probl√®me Identifi√©

**Observation critique** : Le RAG contenait des informations potentiellement obsol√®tes. Par exemple :
- Dates d'√©v√©nements pass√©es
- Anciens contacts ou responsables
- Informations sur les programmes modifi√©es

**Risque** : Fournir des informations incorrectes ou p√©rim√©es aux utilisateurs.

### Solution : Scraper Web

**Objectif** : Compl√©ter le RAG avec des informations fra√Æches du site officiel ESILV.

**Cr√©ation de** [`src/app/api/scraper/route.ts`](../src/app/api/scraper/route.ts)

### Impl√©mentation Initiale (Regex-based)

**Approche** : Extraction basique par expressions r√©guli√®res
```typescript
private extractNewsFromHTML(html: string): ScraperResult[] {
  const titleRegex = /<h5[^>]*><a[^>]*>([^<]+)<\/a><\/h5>/gi
  const dateRegex = /<div class="post_date"[^>]*>([^<]+)<\/div>/gi
  const contentRegex = /<div class="post_excerpt"[^>]*><p>([^<]+)<\/p>/gi
  
  // Extraction avec regex...
}
```

### Limitations de l'Approche Regex

1. **Fragilit√©** : Sensible aux changements du HTML
2. **Complexit√©** : Regex difficiles √† maintenir pour HTML imbriqu√©
3. **Erreurs** : Parsing incomplet si structure l√©g√®rement diff√©rente
4. **Performances** : Lent pour HTML complexe

### R√©sultats

- ‚úÖ Concept valid√© : le scraping apporte de la valeur
- ‚ùå Impl√©mentation fragile et peu fiable
- üîÑ N√©cessit√© de refactoring vers une solution robuste

---

## Phase 5 : Migration JSDOM ‚Üí Cheerio

### Probl√®me Critique

**Erreur rencontr√©e** :
```
Error [ERR_REQUIRE_ESM]: require() of ES Module 
E:\llmgenaip\node_modules\parse5\dist\index.js 
from E:\llmgenaip\node_modules\jsdom\lib\jsdom\browser\parser\html.js 
not supported.
```

**Cause** : JSDOM est un module ESM (ECMAScript Module) qui utilise `parse5`, incompatible avec l'environnement CommonJS des Next.js API Routes.

### Analyse des Alternatives

| Solution | Avantages | Inconv√©nients | D√©cision |
|----------|-----------|---------------|----------|
| **JSDOM** | API compl√®te (DOM Browser) | Erreur ESM, lourd, lent | ‚ùå Rejet√© |
| **Cheerio** | L√©ger, rapide, jQuery-like | Pas de JavaScript rendering | ‚úÖ **Choisi** |
| **Puppeteer** | JavaScript complet, screenshots | Tr√®s lourd, besoin de Chrome | ‚ùå Overkill |
| **jsdom/esm** | R√©soudre le probl√®me ESM | Configuration complexe | ‚ùå Trop complexe |

### Migration vers Cheerio

**Installation** :
```bash
npm uninstall jsdom @types/jsdom
npm install cheerio
```

**Refactoring complet du code de scraping** :

```typescript
import * as cheerio from 'cheerio'

private extractNewsFromHTML(html: string): ScraperResult[] {
  const $ = cheerio.load(html)  // Parser le HTML
  const results: ScraperResult[] = []
  
  // S√©lecteurs CSS pr√©cis pour la structure ESILV
  $('.post_wrapper').each((i, el) => {
    const wrapper = $(el)
    
    // Extraction avec s√©lecteurs jQuery-like
    const titleLink = wrapper.find('.post_header h5 a')
    const title = titleLink.attr('title') || titleLink.text().trim()
    const url = titleLink.attr('href') || ''
    
    const dateDiv = wrapper.find('.post_date')
    const day = dateDiv.find('.date').text().trim()
    const month = dateDiv.find('.month').text().trim()
    const year = dateDiv.find('.year').text().trim()
    const newsDate = `${day} ${month} ${year}`
    
    const excerpt = wrapper.find('.post_excerpt p').text().trim()
    
    const tags: string[] = []
    wrapper.find('.post_detail_item a[rel="tag"]').each((j, tagEl) => {
      tags.push($(tagEl).text().trim())
    })
    
    if (title && url) {
      results.push({ title, content: excerpt, url, confidence: 0.80, date: newsDate, tags })
    }
  })
  
  return results
}
```

### Structure HTML ESILV Cibl√©e

Le site ESILV utilise une structure HTML sp√©cifique :

```html
<div class="post_wrapper">
  <div class="post_header">
    <h5><a href="/actualites/article-url/" title="Titre de l'article">Titre</a></h5>
  </div>
  <div class="post_date">
    <span class="date">10</span>
    <span class="month">D√©c</span>
    <span class="year">2025</span>
  </div>
  <div class="post_excerpt">
    <p>Extrait de l'article...</p>
  </div>
  <div class="post_detail_item">
    <a href="/tag/cybersecurite/" rel="tag">Cybers√©curit√©</a>
    <a href="/tag/hackathon/" rel="tag">Hackathon</a>
  </div>
</div>
```

### R√©sultats

- ‚úÖ Compatibilit√© parfaite avec Next.js API Routes
- ‚úÖ Performance excellente (parsing tr√®s rapide)
- ‚úÖ Code maintenable et lisible
- ‚úÖ Extraction pr√©cise de tous les √©l√©ments (titre, date, tags, URL)
- ‚úÖ Robustesse face aux changements mineurs du HTML

---

## Phase 6 : Deep Scraping

### Besoin Utilisateur

> "il faut que l'agent scrapper r√©cup√®re plus de donn√©es, id√©alement cliquer sur le lien puis scanne la page en question (html) trouve les informations importantes puis les r√©cup√®re et les retournes pour avoir l'avis de l'orchestrateur"

**Probl√®me** : Le scraper ne r√©cup√©rait que les extraits courts (preview) des actualit√©s, pas le contenu complet.

### Solution : Navigation et Extraction en Profondeur

**Ajout du param√®tre `deepScrape`** dans la fonction principale :

```typescript
async scrapeESILVInfo(
  query: string, 
  currentDate?: Date, 
  deepScrape: boolean = true  // ‚Üê Nouveau param√®tre
): Promise<ScraperResult[]>
```

**Impl√©mentation de `scrapeArticlePage()`** :

```typescript
private async scrapeArticlePage(articleUrl: string): Promise<string> {
  try {
    const response = await fetch(articleUrl, {
      headers: { 'User-Agent': 'Mozilla/5.0 (compatible; ESILVBot/1.0)' }
    })
    
    if (!response.ok) return ''
    
    const html = await response.text()
    const $ = cheerio.load(html)
    
    let content = ''
    const paragraphs: string[] = []
    
    // Extraction cibl√©e du contenu principal
    $('div.post_content p').each((i, el) => {
      const cleanParagraph = $(el).text().trim()
      if (cleanParagraph.length > 50) {  // Filtrer le bruit
        paragraphs.push(cleanParagraph)
      }
    })
    
    // Limiter √† 5 paragraphes les plus pertinents
    content = paragraphs.slice(0, 5).join(' ')
    
    // Limitation de longueur pour √©viter surcharge
    if (content.length > 1500) {
      content = content.substring(0, 1500) + '...'
    }
    
    return content
  } catch (error) {
    console.error(`Error scraping article page: ${error}`)
    return ''
  }
}
```

**Int√©gration dans le flux** :

```typescript
private async scrapeNewsPage(currentDate?: Date, deepScrape: boolean = true): Promise<ScraperResult[]> {
  // 1. R√©cup√©rer la page des actualit√©s
  const html = await this.fetchWithRetry('https://www.esilv.fr/actualites/')
  
  // 2. Extraire les 3-6 derniers articles
  const newsItems = this.extractNewsFromHTML(html, currentDate)
  
  // 3. Si deepScrape activ√©, visiter chaque article
  if (deepScrape) {
    for (const item of newsItems) {
      console.log(`üîç Deep scraping: ${item.url}`)
      const fullContent = await this.scrapeArticlePage(item.url)
      if (fullContent) {
        item.fullContent = fullContent  // Ajouter le contenu complet
      }
    }
  }
  
  return newsItems
}
```

### Extraction de 3-6 Articles en Ordre Chronologique

**Logique de limitation** :

```typescript
let newsExtracted = 0
postWrappers.each((i, el) => {
  if (newsExtracted >= 6) return  // Limiter √† 6 articles maximum
  
  // ... extraction ...
  
  if (title && title.length > 20 && !isGeneric && articleUrl) {
    results.push({ /* ... */ })
    newsExtracted++
  }
})
```

**Filtrage des titres g√©n√©riques** :
```typescript
const isGeneric = /^(en savoir plus|demandez|nos brochures|contactez|t√©l√©charger|√©v√©nement)/i.test(title)
```

### R√©sultats

- ‚úÖ Contenu complet des articles (plus seulement les extraits)
- ‚úÖ Extraction de 3-6 derniers articles (chronologiquement)
- ‚úÖ Filtrage des contenus non pertinents
- ‚úÖ Performance acceptable (scraping s√©quentiel avec limitation)
- ‚úÖ Richesse d'informations pour l'orchestrateur

---

## Phase 7 : Navigation Intelligente

### Probl√®me

Le scraper √©tait limit√© aux actualit√©s. Mais les utilisateurs posent aussi des questions sur :
- Les anciens √©l√®ves (alumni)
- Les stages et l'alternance
- Les salaires et d√©bouch√©s
- Les admissions
- Les majeures
- La recherche
- Etc.

**Limitation** : Ces informations ne sont pas dans les actualit√©s, mais sur des pages d√©di√©es.

### Solution : Mapping Query ‚Üí Page Sp√©cifique

**Impl√©mentation d'un dictionnaire intelligent** :

```typescript
private readonly pageMapping: { [key: string]: string } = {
  'alumni|anciens|dipl√¥m√©s|r√©seau': '/entreprises-debouches/reseau-des-anciens/',
  'stage|stages': '/entreprises-debouches/stages-ingenieurs/',
  'alternance|apprentissage': '/entreprises-debouches/filieres-en-alternance/',
  'salaire|emploi|d√©bouch√©|premier emploi': '/entreprises-debouches/enquete-premier-emploi-ingenieur/',
  'admission|concours|avenir': '/admissions/',
  'majeure|sp√©cialisation|cycle ing√©nieur': '/formations/cycle-ingenieur/majeures/',
  'campus|vie √©tudiante|associations': '/lecole/vie-etudiante/',
  'international|√©tranger|erasmus': '/international/',
  'recherche|professeur|devinci research': '/recherche/',
}
```

**Fonction `scrapeSpecificPage()`** :

```typescript
private async scrapeSpecificPage(pagePath: string, query: string): Promise<ScraperResult | null> {
  const fullUrl = `${this.baseUrl}${pagePath.startsWith('/') ? '' : '/'}${pagePath}`
  console.log(`üåê Scraping page sp√©cifique: ${fullUrl} pour la requ√™te: "${query}"`)
  
  try {
    const response = await fetch(fullUrl, {
      headers: { 'User-Agent': 'Mozilla/5.0 (compatible; ESILVBot/1.0)' }
    })
    
    if (!response.ok) return null
    
    const html = await response.text()
    const $ = cheerio.load(html)
    
    let content = ''
    let title = $('h1').first().text().trim() || $('title').text().trim()
    
    // Extraction du contenu pertinent
    const relevantElements = $('h1, h2, h3, h4, p, li')
    relevantElements.each((i, el) => {
      const text = $(el).text().trim()
      if (text.length > 20 && text.length < 500) {  // Filtrer le bruit
        content += text + ' '
      }
    })
    
    content = content.trim().substring(0, 800)  // Limitation
    
    if (content) {
      return {
        title: title || `Information sur ${query}`,
        content: content,
        url: fullUrl,
        confidence: 0.90,  // Haute confiance (page officielle cibl√©e)
        category: 'informations_specifiques_scrap√©es'
      }
    }
    
    return null
  } catch (error) {
    console.error(`Error scraping specific page ${fullUrl}:`, error)
    return null
  }
}
```

**Logique de d√©tection** :

```typescript
async scrapeESILVInfo(query: string, currentDate?: Date, deepScrape: boolean = true): Promise<ScraperResult[]> {
  const results: ScraperResult[] = []
  const lowerQuery = query.toLowerCase()
  
  // 1. V√©rifier si la query correspond √† une page sp√©cifique
  let targetPage: string | null = null
  for (const [keywords, page] of Object.entries(this.pageMapping)) {
    const keywordList = keywords.split('|')
    if (keywordList.some(kw => lowerQuery.includes(kw))) {
      targetPage = page
      console.log(`üéØ Page cible d√©tect√©e: ${page} (mots-cl√©s: ${keywords})`)
      break
    }
  }
  
  // 2. Si page sp√©cifique trouv√©e, la scraper
  if (targetPage) {
    const specificResult = await this.scrapeSpecificPage(targetPage, query)
    if (specificResult) {
      results.push(specificResult)
    }
  }
  
  // 3. Si actualit√© demand√©e ou pas de page sp√©cifique, scraper les news
  const isNewsQuery = /\b(actualit√©|actualit√©s|news|dernier|derni√®re|r√©cent|nouveau)\b/i.test(query)
  if (isNewsQuery || !targetPage) {
    const newsResults = await this.scrapeNewsPage(currentDate, deepScrape)
    results.push(...newsResults)
  }
  
  return results
}
```

### Exemples de Mapping

| Requ√™te Utilisateur | Mots-cl√©s D√©tect√©s | Page Cibl√©e |
|---------------------|-------------------|-------------|
| "Qui est la responsable alumni ?" | alumni | `/entreprises-debouches/reseau-des-anciens/` |
| "Quels sont les stages disponibles ?" | stage | `/entreprises-debouches/stages-ingenieurs/` |
| "Comment fonctionne l'alternance ?" | alternance | `/entreprises-debouches/filieres-en-alternance/` |
| "Quel est le salaire moyen des dipl√¥m√©s ?" | salaire, emploi | `/entreprises-debouches/enquete-premier-emploi-ingenieur/` |
| "Quelles sont les derni√®res actualit√©s ?" | actualit√©s | `/actualites/` (scraping news) |

### R√©sultats

- ‚úÖ Couverture de 9 cat√©gories de pages importantes
- ‚úÖ D√©tection intelligente bas√©e sur les mots-cl√©s
- ‚úÖ Haute confiance (0.90) pour les pages officielles cibl√©es
- ‚úÖ Fallback sur actualit√©s si pas de page sp√©cifique
- ‚úÖ Performance optimis√©e (scraping uniquement de la page pertinente)

---

## Phase 8 : Persistance de l'Historique

### Probl√®me

**Observation utilisateur** : "Le chatbot perd toute la conversation quand je recharge la page ou que je reviens plus tard."

**Impact** :
- Exp√©rience utilisateur frustrante
- Obligation de tout r√©expliquer
- Perte de contexte pour les questions de suivi

### Solution : localStorage avec SessionManager

**Impl√©mentation dans** [`src/app/page.tsx`](../src/app/page.tsx)

**Cr√©ation du SessionManager** (utilitaire) :

```typescript
// src/lib/sessionManager.ts
export class SessionManager {
  private static readonly USER_ID_KEY = 'esilv_chatbot_user_id'
  private static readonly SESSION_ID_KEY = 'esilv_chatbot_session_id'
  private static readonly MESSAGES_KEY = 'esilv_chatbot_messages'
  
  static getUserId(): string {
    let userId = localStorage.getItem(this.USER_ID_KEY)
    if (!userId) {
      userId = `user_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
      localStorage.setItem(this.USER_ID_KEY, userId)
    }
    return userId
  }
  
  static getSessionId(): string {
    let sessionId = localStorage.getItem(this.SESSION_ID_KEY)
    if (!sessionId) {
      sessionId = `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
      localStorage.setItem(this.SESSION_ID_KEY, sessionId)
    }
    return sessionId
  }
  
  static saveMessages(messages: Message[]): void {
    localStorage.setItem(this.MESSAGES_KEY, JSON.stringify(messages))
  }
  
  static loadMessages(): Message[] {
    const saved = localStorage.getItem(this.MESSAGES_KEY)
    return saved ? JSON.parse(saved) : []
  }
  
  static clearSession(): void {
    localStorage.removeItem(this.SESSION_ID_KEY)
    localStorage.removeItem(this.MESSAGES_KEY)
  }
}
```

**Int√©gration dans le composant Chat** :

```typescript
'use client'

import { useEffect, useState } from 'react'
import { SessionManager } from '@/lib/sessionManager'

export default function ChatPage() {
  const [messages, setMessages] = useState<Message[]>([])
  const [userId, setUserId] = useState<string>('')
  const [sessionId, setSessionId] = useState<string>('')
  
  // Charger l'historique au montage du composant
  useEffect(() => {
    const loadedMessages = SessionManager.loadMessages()
    setMessages(loadedMessages)
    setUserId(SessionManager.getUserId())
    setSessionId(SessionManager.getSessionId())
  }, [])
  
  // Sauvegarder l'historique √† chaque changement
  useEffect(() => {
    if (messages.length > 0) {
      SessionManager.saveMessages(messages)
    }
  }, [messages])
  
  const handleNewMessage = async (content: string) => {
    const newMessage: Message = {
      id: `msg_${Date.now()}`,
      role: 'user',
      content,
      timestamp: new Date()
    }
    
    setMessages(prev => [...prev, newMessage])
    
    // ... appel API et ajout de la r√©ponse ...
  }
  
  const handleClearHistory = () => {
    SessionManager.clearSession()
    setMessages([])
    setSessionId(SessionManager.getSessionId())  // Nouveau session
  }
  
  return (
    <div>
      <ChatInterface 
        messages={messages}
        onSendMessage={handleNewMessage}
        onClearHistory={handleClearHistory}
      />
    </div>
  )
}
```

### Fonctionnalit√©s Ajout√©es

1. **G√©n√©ration automatique d'IDs uniques** :
   - `userId` : Identifiant persistant de l'utilisateur
   - `sessionId` : Identifiant de la session conversationnelle

2. **Sauvegarde automatique** :
   - Chaque message est imm√©diatement sauvegard√© dans `localStorage`
   - D√©clenchement automatique via `useEffect`

3. **Chargement au d√©marrage** :
   - R√©cup√©ration de l'historique complet au montage du composant
   - Restauration de l'√©tat conversationnel

4. **Bouton "Nouvelle conversation"** :
   - Efface l'historique local
   - G√©n√®re un nouveau `sessionId`
   - Conserve le `userId` pour tracking utilisateur

### R√©sultats

- ‚úÖ Persistance compl√®te de l'historique conversationnel
- ‚úÖ Rechargement de page sans perte de contexte
- ‚úÖ Reprise de conversation apr√®s fermeture/r√©ouverture du navigateur
- ‚úÖ Gestion propre des nouvelles conversations
- ‚úÖ Tracking utilisateur pour analytics futurs

---

## Phase 9 : RAG Conflict Resolution

### Probl√®me Critique Identifi√©

**Incident d√©clencheur** : 

Utilisateur pose la question : *"Qui est la responsable alumni de l'esilv ?"*

**R√©ponse du chatbot** : "La responsable alumni est [Ancien Nom]"

**V√©rification sur le site** : Le nom a chang√© depuis plusieurs mois. La responsable actuelle est diff√©rente.

**Retour utilisateur** :
> "es tu sur ?"  
> "la reponsable a chang√© et est dispo sur cette page https://www.esilv.fr/entreprises-debouches/reseau-des-anciens/ prends en compte qu'il va falloir maintenant supprimer les infos du rag si une infos inverse est trouv√©e sur le site"

### Analyse du Probl√®me

**Cause racine** : Le RAG contient des donn√©es statiques qui ne sont jamais mises √† jour automatiquement.

**Types d'informations √† risque** :
- Personnel et contacts (noms, titres, emails, t√©l√©phones)
- Dates d'√©v√©nements
- Statistiques (salaires, taux d'emploi, etc.)
- Programmes et majeures (√©volutions, fermetures, nouvelles ouvertures)

**Gravit√©** : Tr√®s haute ‚Üí Perte de confiance des utilisateurs, informations incorrectes diffus√©es

### Exigence Utilisateur

> "il faut activer le scrapper tout le temps ! le principe est qu'il fait une recherche en parall√®le si il le faut pour v√©rifier constamment chaque infos d√Ætes (plus une info est vieille plus elle doit √™tre soumise √† v√©rification) par exemple des donn√©es scrapp√©es aujourd'hui n'ont pas besoin d'√™tre rev√©rifi√©e"

**Contraintes** :
1. Le scraper doit √™tre actif en permanence pour v√©rification
2. V√©rification bas√©e sur l'√¢ge des donn√©es
3. Donn√©es r√©centes (scrap√©es aujourd'hui) : pas de re-v√©rification
4. Donn√©es anciennes : v√©rification automatique
5. Si conflit d√©tect√© : **suppression automatique** des donn√©es obsol√®tes du RAG

### Solution Architecturale : V√©rification Parall√®le Intelligente

#### 1. Ajout du Champ `lastVerified`

**Modification de** [`prisma/schema.prisma`](../prisma/schema.prisma) :

```prisma
model KnowledgeBase {
  id            String   @id @default(cuid())
  question      String
  answer        String
  category      String
  confidence    Float?
  source        String?
  lastVerified  DateTime @default(now())  // ‚Üê NOUVEAU : Tracking de fra√Æcheur
  createdAt     DateTime @default(now())
  updatedAt     DateTime @updatedAt
}
```

**Migration** :
```bash
npx prisma db push
```

#### 2. R√®gles de V√©rification Intelligente

**Impl√©mentation dans** [`src/app/api/chat/route.ts`](../src/app/api/chat/route.ts) :

```typescript
async handleRetrieval(message: string, conversationHistory: any[] = []): Promise<{ response: string; agentType: AgentType }> {
  await this.initialize()
  
  const currentDate = new Date()
  
  // D√©tection des types de questions n√©cessitant v√©rification
  const needsRecentInfo = /\b(dernier|derni√®re|r√©cent|r√©cente|nouveau|nouvelle|actualit√©|actualit√©s)\b/i.test(message)
  const needsWebVerification = /\b(responsable|contact|directeur|directrice|chef|manager|personnel|√©quipe|qui est|t√©l√©phone|email|adresse)\b/i.test(message)
  
  let knowledgeResults = ''
  let sources: any[] = []
  let needsVerification = false
  
  // 1. TOUJOURS interroger le RAG en premier (r√©ponse rapide)
  const ragData = await this.searchKnowledgeBase(message)
  knowledgeResults = ragData.results
  sources = ragData.sources
  
  // 2. V√©rifier l'√¢ge des donn√©es RAG
  if (sources.length > 0) {
    const oldestSource = sources[0]
    const lastVerified = oldestSource.lastVerified 
      ? new Date(oldestSource.lastVerified) 
      : new Date(oldestSource.createdAt)
    const daysSinceVerification = Math.floor(
      (currentDate.getTime() - lastVerified.getTime()) / (1000 * 60 * 60 * 24)
    )
    
    // R√àGLES DE V√âRIFICATION
    if (daysSinceVerification > 30) {
      // Donn√©es > 30 jours : v√©rification syst√©matique
      needsVerification = true
      console.log(`‚ö†Ô∏è Donn√©es RAG anciennes (${daysSinceVerification} jours) ‚Üí V√©rification scraper n√©cessaire`)
    } else if (daysSinceVerification > 7 && (needsRecentInfo || needsWebVerification)) {
      // Donn√©es > 7 jours + question sensible : v√©rification
      needsVerification = true
      console.log(`‚ö†Ô∏è Donn√©es RAG de ${daysSinceVerification} jours + question sensible ‚Üí V√©rification scraper`)
    } else {
      console.log(`‚úÖ Donn√©es RAG r√©centes (${daysSinceVerification} jours) ‚Üí Pas de v√©rification n√©cessaire`)
    }
  }
  
  // 3. Activer le scraper si n√©cessaire
  if (needsRecentInfo || needsWebVerification || needsVerification || !knowledgeResults) {
    let reason = 'fallback (RAG vide)'
    if (needsRecentInfo) reason = 'actualit√©s'
    else if (needsWebVerification) reason = 'informations variables (personnel/contacts)'
    else if (needsVerification) reason = 'v√©rification donn√©es anciennes'
    
    console.log(`üåê Scraper activ√©: ${reason}`)
    
    // Lancer le scraper EN PARALL√àLE si on a d√©j√† des donn√©es RAG
    if (knowledgeResults && knowledgeResults.trim() !== '') {
      console.log('üîÑ Scraping en parall√®le pour v√©rification...')
      
      // Scraper en arri√®re-plan (ne pas attendre)
      this.searchWebESILV(message, currentDate).then(async (webData) => {
        if (webData && webData.trim() !== '') {
          console.log('‚úÖ Scraper termin√© - Comparaison avec RAG...')
          // TODO: Comparer webData avec knowledgeResults et mettre √† jour si diff√©rent
          console.log('üìä Donn√©es web disponibles pour comparaison')
        }
      }).catch(err => console.error('‚ùå Erreur scraper parall√®le:', err))
      
      // Utiliser les donn√©es RAG imm√©diatement (pas d'attente)
      console.log('‚ö° R√©ponse imm√©diate avec donn√©es RAG (scraper en arri√®re-plan)')
    } else {
      // Pas de donn√©es RAG, attendre le scraper
      const webResults = await this.searchWebESILV(message, currentDate)
      console.log(`‚úÖ Scraper termin√©: ${reason}`)
      // Utiliser webResults pour la r√©ponse
    }
  }
  
  // ... g√©n√©ration de la r√©ponse avec l'IA ...
}
```

**R√©sum√© des R√®gles** :

| √Çge des Donn√©es | Type de Question | Action | Raison |
|-----------------|------------------|--------|--------|
| < 7 jours | Toute | Pas de v√©rification | Donn√©es r√©centes fiables |
| 7-30 jours | G√©n√©rale | Pas de v√©rification | Donn√©es encore valides |
| 7-30 jours | Actualit√©s / Personnel | V√©rification parall√®le | Risque de changement |
| > 30 jours | Toute | V√©rification parall√®le | Donn√©es potentiellement obsol√®tes |
| N/A | Actualit√©s explicites | V√©rification imm√©diate | Toujours √† jour |
| N/A | Personnel/Contacts | V√©rification imm√©diate | Informations variables |

#### 3. Scraping Parall√®le : UX Non Bloqu√©e

**Principe** : 
1. L'utilisateur re√ßoit une r√©ponse **imm√©diate** bas√©e sur le RAG
2. Le scraper s'ex√©cute en **arri√®re-plan** (async)
3. Si conflit d√©tect√©, le RAG sera mis √† jour pour les **prochaines** requ√™tes

**Avantages** :
- ‚úÖ Temps de r√©ponse < 2 secondes (pas d'attente du scraper)
- ‚úÖ V√©rification continue sans impact UX
- ‚úÖ Donn√©es progressivement actualis√©es
- ‚úÖ Scalabilit√© (scraper ne ralentit pas l'API)

**Code** :
```typescript
// Lancer en parall√®le (async, pas de await)
this.searchWebESILV(message, currentDate).then(async (webData) => {
  // Traitement asynchrone de la v√©rification
  if (webData) {
    // Comparaison et mise √† jour du RAG si n√©cessaire
  }
}).catch(err => console.error('‚ùå Erreur scraper parall√®le:', err))

// Continuer imm√©diatement avec la r√©ponse RAG
console.log('‚ö° R√©ponse imm√©diate avec donn√©es RAG')
```

#### 4. API de D√©tection de Conflits

**Actions ajout√©es dans** [`src/app/api/knowledge/route.ts`](../src/app/api/knowledge/route.ts) :

##### a) Action `find_conflicts`

```typescript
case 'find_conflicts': {
  const { newInfo } = body
  if (!newInfo) {
    return NextResponse.json({ error: 'newInfo is required' }, { status: 400 })
  }
  
  // Extraire les mots-cl√©s de la nouvelle information
  const newInfoKeywords = this.extractKeywords(newInfo)
  
  if (newInfoKeywords.length === 0) {
    return NextResponse.json({ conflicts: [], count: 0 })
  }
  
  // Comparer avec toutes les entr√©es RAG
  const allKnowledge = await db.knowledgeBase.findMany()
  const conflicts: any[] = []
  
  for (const entry of allKnowledge) {
    const entryText = `${entry.question} ${entry.answer}`.toLowerCase()
    const sharedKeywords = newInfoKeywords.filter(keyword => 
      entryText.includes(keyword.toLowerCase())
    )
    
    // Si mots-cl√©s partag√©s mais contenu diff√©rent = conflit potentiel
    if (sharedKeywords.length > 0 && !newInfo.toLowerCase().includes(entry.answer.toLowerCase())) {
      conflicts.push({
        id: entry.id,
        question: entry.question,
        answer: entry.answer,
        category: entry.category,
        sharedKeywords: sharedKeywords
      })
    }
  }
  
  return NextResponse.json({ 
    success: true, 
    conflicts, 
    count: conflicts.length 
  })
}

private extractKeywords(text: string): string[] {
  // Mots vides √† ignorer
  const stopWords = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'ou', 'est', 'sont']
  
  return text
    .toLowerCase()
    .replace(/[^\w\s]/gi, '')  // Retirer ponctuation
    .split(/\s+/)              // S√©parer en mots
    .filter(word => word.length > 3 && !stopWords.includes(word))  // Filtrer
    .slice(0, 20)              // Limiter √† 20 mots-cl√©s
}
```

##### b) Action `delete_by_keywords`

```typescript
case 'delete_by_keywords': {
  const { keywords } = body
  if (!keywords) {
    return NextResponse.json({ error: 'keywords are required' }, { status: 400 })
  }
  
  const keywordList = Array.isArray(keywords) ? keywords : keywords.split(' ')
  
  // Construire les conditions de suppression
  const deleteConditions = keywordList.flatMap((keyword: string) => [
    { question: { contains: keyword, mode: 'insensitive' as const } },
    { answer: { contains: keyword, mode: 'insensitive' as const } }
  ])
  
  // Supprimer toutes les entr√©es contenant ces mots-cl√©s
  const deletedEntries = await db.knowledgeBase.deleteMany({
    where: {
      OR: deleteConditions
    }
  })
  
  return NextResponse.json({ 
    success: true, 
    deleted: deletedEntries.count, 
    keywords 
  })
}
```

### √âtat Actuel : v1.0.0-stable

**Ce qui est impl√©ment√©** :
- ‚úÖ Champ `lastVerified` dans le sch√©ma Prisma
- ‚úÖ R√®gles de v√©rification intelligente bas√©es sur l'√¢ge
- ‚úÖ D√©tection automatique des questions sensibles (personnel, actualit√©s)
- ‚úÖ Scraping parall√®le non bloquant
- ‚úÖ API `find_conflicts` pour d√©tecter les conflits
- ‚úÖ API `delete_by_keywords` pour suppression cibl√©e
- ‚úÖ Logging complet des op√©rations de v√©rification

**Ce qui reste √† impl√©menter (v1.1.0 pr√©vue)** :
- üöß Comparaison automatique `webData` vs `knowledgeResults`
- üöß D√©clenchement automatique de la d√©tection de conflits
- üöß Mise √† jour automatique du RAG (suppression + ajout)
- üöß Logging des mises √† jour dans une table `RAGUpdate`
- üöß Interface admin pour visualiser l'historique des mises √† jour

**Ligne de code √† impl√©menter** (ligne 155 dans `chat/route.ts`) :
```typescript
// TODO: Comparer webData avec knowledgeResults et mettre √† jour si diff√©rent
```

### Diagramme : Flux Actuel RAG + Scraper

```mermaid
sequenceDiagram
    participant User
    participant Orchestrator
    participant RAG
    participant Scraper
    participant Website
    
    User->>Orchestrator: Question
    Orchestrator->>RAG: Recherche (rapide)
    RAG-->>Orchestrator: R√©sultats + lastVerified
    
    alt Donn√©es anciennes ou question sensible
        Orchestrator->>User: R√©ponse imm√©diate (RAG)
        Orchestrator->>Scraper: V√©rification parall√®le (async)
        Scraper->>Website: Fetch + Parse (Cheerio)
        Website-->>Scraper: HTML
        Scraper-->>Orchestrator: Donn√©es fra√Æches
        Note over Orchestrator: TODO: Comparaison + Mise √† jour RAG
    else Donn√©es r√©centes
        Orchestrator->>User: R√©ponse (RAG uniquement)
    end
```

---

## D√©cisions Techniques Cl√©s

### 1. Next.js API Routes vs Express

**Choix** : Next.js API Routes

**Justification** :
- **Simplicit√©** : Endpoints API dans le m√™me projet que le frontend
- **Performance** : Optimisations Next.js (caching, ISR, SSR)
- **D√©ploiement** : D√©ploiement unique sur Vercel (frontend + backend)
- **TypeScript** : Support natif et excellent
- **Co√ªt** : Free tier g√©n√©reux de Vercel

**Alternative rejet√©e** : Express standalone
- N√©cessiterait deux d√©ploiements s√©par√©s
- Configuration CORS plus complexe
- Moins d'optimisations automatiques

### 2. SQLite vs PostgreSQL

**Choix** : SQLite

**Justification** :
- **L√©g√®ret√©** : Base de donn√©es embarqu√©e, aucun serveur √† g√©rer
- **Portabilit√©** : Fichier unique `.db` facilement sauvegardable
- **Performance suffisante** : Pour un chatbot avec < 1000 requ√™tes/jour
- **Simplicit√©** : Pas de configuration, pas de credentials
- **D√©veloppement local** : Identique √† production

**Alternative rejet√©e** : PostgreSQL
- Overkill pour le scope actuel
- N√©cessiterait h√©bergement s√©par√©
- Co√ªts suppl√©mentaires
- Configuration plus complexe

**√âvolution future** : Migration vers PostgreSQL si :
- Besoin de recherche vectorielle (pgvector)
- Croissance significative du trafic (> 10K req/jour)
- Besoin de r√©plication / haute disponibilit√©

### 3. Cheerio vs JSDOM vs Puppeteer

**Choix** : Cheerio

**Justification** :
- **Performance** : 10x plus rapide que JSDOM
- **Compatibilit√©** : Fonctionne parfaitement avec Next.js API Routes (CommonJS)
- **API** : jQuery-like, famili√®re et intuitive
- **L√©g√®ret√©** : ~ 1MB vs 20MB+ pour JSDOM/Puppeteer
- **Cas d'usage** : Le site ESILV ne n√©cessite pas de JavaScript rendering

**Alternatives rejet√©es** :
- **JSDOM** : Erreur `ERR_REQUIRE_ESM`, trop lourd
- **Puppeteer** : Overkill (besoin de Chrome/Chromium), tr√®s lourd, lent

**Limite accept√©e** : Cheerio ne peut pas ex√©cuter JavaScript. Acceptable car le site ESILV g√©n√®re son HTML c√¥t√© serveur.

### 4. Scraping Parall√®le vs Bloquant

**Choix** : Scraping Parall√®le (async)

**Justification** :
- **UX prioritaire** : R√©ponse imm√©diate < 2 secondes
- **Donn√©es fra√Æches en arri√®re-plan** : V√©rification continue sans impact utilisateur
- **Scalabilit√©** : Le scraper ne ralentit pas l'API principale
- **Coh√©rence** : R√©ponse bas√©e sur une source unique (RAG ou Web, pas mixte)

**Alternative rejet√©e** : Scraping bloquant
- Temps de r√©ponse 5-15 secondes (inacceptable)
- Mauvaise UX, perception de lenteur
- Surcharge serveur si trafic √©lev√©

**Trade-off accept√©** : La premi√®re r√©ponse peut √™tre bas√©e sur des donn√©es l√©g√®rement obsol√®tes, mais la prochaine fois (apr√®s v√©rification parall√®le) sera √† jour.

### 5. localStorage vs Sessions Serveur

**Choix** : localStorage (client-side)

**Justification** :
- **Simplicit√©** : Pas besoin de base de donn√©es de sessions
- **Pas d'authentification** : MVP sans compte utilisateur
- **Performances** : Pas de requ√™tes serveur suppl√©mentaires
- **Privacit√©** : Donn√©es restent sur l'appareil de l'utilisateur
- **Co√ªt** : Z√©ro (pas de stockage serveur)

**Alternative rejet√©e** : Sessions serveur
- N√©cessiterait authentification
- Base de donn√©es ou Redis pour sessions
- Co√ªt et complexit√© suppl√©mentaires

**√âvolution future** : Migration vers sessions serveur si :
- Ajout d'authentification utilisateur
- Besoin de synchronisation multi-appareils
- Analytics avanc√©s n√©cessitant historique centralis√©

---

## √âtat Actuel (v1.0.0-stable)

### Fonctionnalit√©s Stables ‚úÖ

| Fonctionnalit√© | Description | Fichiers Principaux |
|----------------|-------------|---------------------|
| **Syst√®me Multi-Agents** | 3 agents sp√©cialis√©s (Orchestration, RAG, Form-filling) | `src/app/api/chat/route.ts` |
| **Base de Connaissances** | 29+ entr√©es, recherche intelligente avec variantes | `src/app/api/knowledge/route.ts` |
| **Support Multi-LLM** | Ollama, Gemini, OpenAI, Claude, HuggingFace | `src/lib/ai-providers/` |
| **Scraper Web Fonctionnel** | Cheerio, extraction pr√©cise, structure ESILV | `src/app/api/scraper/route.ts` |
| **Navigation Intelligente** | Mapping query ‚Üí page (9 cat√©gories) | `src/app/api/scraper/route.ts` |
| **Deep Scraping** | Contenu complet des articles (3-6 derniers) | `src/app/api/scraper/route.ts` |
| **V√©rification Parall√®le** | Scraping async bas√© sur `lastVerified` | `src/app/api/chat/route.ts` |
| **Interface Utilisateur** | Design moderne, responsive, badges agents | `src/app/page.tsx` |
| **Persistance Conversations** | localStorage avec SessionManager | `src/lib/sessionManager.ts` |
| **API Conflits** | `find_conflicts`, `delete_by_keywords` | `src/app/api/knowledge/route.ts` |

### En D√©veloppement üöß

| Fonctionnalit√© | Statut | Prochaine √âtape |
|----------------|--------|----------------|
| **Comparaison Auto RAG vs Web** | Planifi√© | Impl√©menter `compareDataSources()` |
| **D√©tection Conflits Auto** | Planifi√© | Int√©grer API `find_conflicts` dans flux parall√®le |
| **Mise √† Jour Auto RAG** | Planifi√© | Impl√©menter `updateRAGWithWebData()` |
| **Logging Mises √† Jour** | Planifi√© | Cr√©er mod√®le `RAGUpdate` + endpoint |
| **Interface Admin Updates** | Optionnel | Page admin pour visualiser historique |

### M√©triques de Performance

| M√©trique | Valeur Actuelle | Objectif |
|----------|----------------|----------|
| **Temps de R√©ponse RAG** | < 2 secondes | ‚úÖ < 3s |
| **Temps de R√©ponse Scraper** | 3-8 secondes | ‚úÖ < 10s |
| **Temps R√©ponse Parall√®le** | < 2 secondes (user ne voit pas le scraper) | ‚úÖ < 3s |
| **Couverture Questions** | ~95% des questions fr√©quentes | üéØ > 90% |
| **Taux Confiance Moyen** | 0.85 | üéØ > 0.8 |
| **Actualit√© des Donn√©es** | V√©rification bas√©e sur √¢ge (7-30j) | üîÑ En am√©lioration |

### Statistiques Techniques

- **Lignes de Code** : ~8,500 lignes TypeScript
- **Fichiers Principaux** : 25+
- **Endpoints API** : 12
- **Composants React** : 30+
- **Mod√®les Prisma** : 7
- **Pages Documentation** : 4 (README, TECHNICAL_DOC, AI_PROVIDERS, PROJECT_JOURNEY)

---

## √âvolution de l'Architecture

### Diagramme Temporel

```mermaid
graph LR
    V1[v0.1<br/>Chatbot Simple<br/>RAG Statique] --> V2[v0.3<br/>Multi-Agents<br/>3 Agents Sp√©cialis√©s]
    V2 --> V3[v0.5<br/>RAG Enrichi<br/>29+ Entr√©es]
    V3 --> V4[v0.7<br/>Scraper Web<br/>Cheerio + Deep Scraping]
    V4 --> V5[v1.0<br/>V√©rification Parall√®le<br/>lastVerified + Smart Rules]
    V5 -.-> V6[v1.1 Pr√©vu<br/>Auto-Update RAG<br/>Comparaison + Conflits]
    
    style V5 fill:#90EE90
    style V6 fill:#FFD700,stroke-dasharray: 5 5
```

### Architecture Cible (v1.1)

```mermaid
graph TB
    User[Utilisateur] --> Orchestrator[Orchestrateur Chat]
    Orchestrator --> RAG[RAG Database]
    Orchestrator --> Scraper[Web Scraper]
    
    Scraper --> WebsiteESILV[Site ESILV]
    Scraper --> Cheerio[Cheerio Parser]
    
    Orchestrator --> Comparator[Comparateur]
    Comparator --> ConflictDetector[D√©tecteur Conflits]
    ConflictDetector --> Updater[Mise √† Jour RAG]
    
    Updater --> RAG
    Updater --> Logger[Logger RAGUpdate]
    Logger --> AuditDB[Base Audit]
    
    RAG --> User
    AuditDB --> AdminUI[Interface Admin]
    
    style Comparator fill:#FFD700
    style ConflictDetector fill:#FFD700
    style Updater fill:#FFD700
    style Logger fill:#FFD700
    style AdminUI fill:#FFD700
```

**L√©gende** :
- üü¢ Vert : Fonctionnalit√©s stables impl√©ment√©es
- üü° Jaune : Fonctionnalit√©s en d√©veloppement (v1.1)

---

## Conclusion

Ce document retrace l'√©volution compl√®te du projet ESILV Smart Assistant depuis sa gen√®se jusqu'√† la version stable actuelle (v1.0.0-stable). Chaque phase a apport√© des am√©liorations significatives, guid√©es par les besoins utilisateurs et les d√©fis techniques rencontr√©s.

**Le√ßons Apprises** :
1. **Architecture modulaire** : Facilite l'√©volution et la maintenance
2. **Feedback utilisateur** : Essentiel pour orienter les d√©veloppements
3. **Choix technologiques pragmatiques** : Pr√©f√©rer la simplicit√© quand suffisante
4. **It√©rations rapides** : Mieux vaut une solution simple fonctionnelle qu'une solution parfaite incompl√®te
5. **Parall√©lisme** : Am√©liorer l'UX sans sacrifier la qualit√© des donn√©es

**Prochaines √âtapes (v1.1)** :
- Finaliser le syst√®me de mise √† jour automatique du RAG
- Impl√©menter la comparaison et d√©tection de conflits
- Ajouter le logging complet des mises √† jour
- Cr√©er l'interface admin pour audit

---

**Document maintenu par** : √âquipe de d√©veloppement ESILV Smart Assistant  
**Derni√®re mise √† jour** : 4 Janvier 2025  
**Version du projet** : v1.0.0-stable

