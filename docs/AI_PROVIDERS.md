# Configuration Ollama et Gemini pour ESILV Smart Assistant

## ‚úÖ Configuration Compl√®te

Le projet a √©t√© configur√© pour utiliser **Ollama** (local) et **Gemini** (Google) comme fournisseurs IA.

## üîß Configuration du fichier .env

Le fichier `.env` a √©t√© mis √† jour avec les variables suivantes :

```env
# AI Provider Configuration
AI_PROVIDER=ollama  # ou 'gemini' pour utiliser Gemini

# Ollama Configuration (pour LLM local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Gemini Configuration (Google Gen AI)
GEMINI_API_KEY=votre-cl√©-api-gemini
GEMINI_MODEL=gemini-2.0-flash-exp
```

## ü¶ô Configuration Ollama (Local)

### 1. Installation d'Ollama

**Windows :**
- T√©l√©charger depuis : https://ollama.ai/download
- Installer l'ex√©cutable

**Linux/Mac :**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Configuration du r√©pertoire des mod√®les (Windows)

Si vos mod√®les sont dans un r√©pertoire personnalis√© (ex: `E:\ollama_models`) :

**Option 1 : Variable d'environnement syst√®me**
1. Ouvrir "Variables d'environnement" dans Windows
2. Ajouter une nouvelle variable syst√®me : `OLLAMA_MODELS=E:\ollama_models`
3. Red√©marrer Ollama

**Option 2 : Variable d'environnement PowerShell (temporaire)**
```powershell
$env:OLLAMA_MODELS="E:\ollama_models"
ollama serve
```

### 3. D√©marrer Ollama

```bash
# Avec r√©pertoire personnalis√© (si configur√©)
$env:OLLAMA_MODELS="E:\ollama_models"
ollama serve

# Ou normalement
ollama serve
```

### 3. Mod√®les disponibles

Vos mod√®les sont stock√©s dans `E:\ollama_models`. Mod√®les actuellement disponibles :
- `qwen2.5:7b` / `qwen2.5:latest`
- `mistral:7b`
- `ministral-3:latest`
- `llama3:latest`
- `mistral-large-3:675b-cloud`

Pour t√©l√©charger d'autres mod√®les :
```bash
ollama pull llama3.1:8b
ollama pull llama2
ollama pull codellama
```

**Note :** Les mod√®les sont automatiquement sauvegard√©s dans `E:\ollama_models` gr√¢ce √† la variable `OLLAMA_MODELS`.

### 4. V√©rifier que Ollama fonctionne

```bash
ollama list
```

### 5. Configuration dans .env

```env
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

## üåü Configuration Gemini (Google)

### 1. Obtenir une cl√© API

1. Aller sur : https://aistudio.google.com/apikey
2. Cr√©er une nouvelle cl√© API
3. Copier la cl√©

### 2. Configuration dans .env

```env
AI_PROVIDER=gemini
GEMINI_API_KEY=votre-cl√©-api-ici
GEMINI_MODEL=gemini-2.0-flash-exp
```

### 3. Mod√®les Gemini disponibles

- `gemini-2.0-flash-exp` (recommand√© - rapide et gratuit)
- `gemini-1.5-pro` (plus puissant)
- `gemini-1.5-flash` (rapide)
- `gemini-pro` (ancien mod√®le)

## üîÑ Changer de fournisseur

### M√©thode 1 : Modifier .env

1. Ouvrir le fichier `.env`
2. Changer `AI_PROVIDER` :
   - `AI_PROVIDER=ollama` pour Ollama
   - `AI_PROVIDER=gemini` pour Gemini
3. Red√©marrer le serveur : `npm run dev`

### M√©thode 2 : Via l'API

```bash
# Changer vers Ollama
curl -X POST http://localhost:3000/api/ai-config \
  -H "Content-Type: application/json" \
  -d '{"provider": "ollama", "model": "llama3.1:8b"}'

# Changer vers Gemini
curl -X POST http://localhost:3000/api/ai-config \
  -H "Content-Type: application/json" \
  -d '{"provider": "gemini", "model": "gemini-2.0-flash-exp"}'
```

## üß™ Tester la configuration

### Tester Ollama

```bash
# V√©rifier que Ollama r√©pond
curl http://localhost:11434/api/tags

# Tester un mod√®le
ollama run llama3.1:8b "Bonjour, comment √ßa va ?"
```

### Tester Gemini

```bash
# V√©rifier la configuration
curl http://localhost:3000/api/ai-config
```

### Tester le chatbot

1. D√©marrer le serveur : `npm run dev`
2. Ouvrir http://localhost:3000
3. Envoyer un message de test

## üìù Notes importantes

### Ollama
- ‚úÖ **Gratuit** et **local** (donn√©es priv√©es)
- ‚úÖ Pas besoin de cl√© API
- ‚ö†Ô∏è N√©cessite que `ollama serve` soit en cours d'ex√©cution
- ‚ö†Ô∏è N√©cessite assez de RAM (8GB+ recommand√© pour llama3.1:8b)

### Gemini
- ‚úÖ **Rapide** et **puissant**
- ‚úÖ Gratuit jusqu'√† un certain quota
- ‚ö†Ô∏è N√©cessite une cl√© API Google
- ‚ö†Ô∏è Les donn√©es sont envoy√©es √† Google

## üêõ D√©pannage

### Ollama ne r√©pond pas

1. V√©rifier que `ollama serve` est en cours d'ex√©cution
2. V√©rifier l'URL dans `.env` : `OLLAMA_BASE_URL=http://localhost:11434`
3. Tester : `curl http://localhost:11434/api/tags`

### Gemini erreur "API key not found"

1. V√©rifier que `GEMINI_API_KEY` est d√©fini dans `.env`
2. V√©rifier que la cl√© est valide sur https://aistudio.google.com/apikey
3. Red√©marrer le serveur apr√®s modification de `.env`

### Le chatbot ne r√©pond pas

1. V√©rifier les logs du serveur pour voir les erreurs
2. V√©rifier la configuration : `curl http://localhost:3000/api/ai-config`
3. Tester avec un autre fournisseur

## üöÄ D√©marrage rapide

```bash
# 1. Installer Ollama (si pas d√©j√† fait)
# Windows : t√©l√©charger depuis https://ollama.ai/download

# 2. D√©marrer Ollama
ollama serve

# 3. T√©l√©charger un mod√®le
ollama pull llama3.1:8b

# 4. Configurer .env (d√©j√† fait)
# AI_PROVIDER=ollama
# OLLAMA_MODEL=llama3.1:8b

# 5. D√©marrer le serveur Next.js
npm run dev

# 6. Tester
# Ouvrir http://localhost:3000
```

---

**Le projet est maintenant configur√© pour utiliser Ollama et Gemini !** üéâ

